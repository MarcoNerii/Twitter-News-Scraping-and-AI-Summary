{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fce42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "from zoneinfo import ZoneInfo\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# --- Scraper settings ---\n",
    "USER = \"financialjuice\"          # handle to scrape\n",
    "HOURS_BACK = 24                  # last N hours\n",
    "OUT_TXT = \"financialjuice_last_hours.txt\"\n",
    "OUTPUT_TZ = \"Europe/Zurich\"      # Geneva time\n",
    "MAX_SCROLLS = 80                 # increase for more tweets\n",
    "SCROLL_WAIT_MS = 1600            # increase if loading is slow (e.g., 2000+)\n",
    "COOKIES_FILE = \"x_cookies.json\"  # optional: export your x.com cookies to this file\n",
    "\n",
    "UA = (\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "    \"Chrome/124.0 Safari/537.36\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029cb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A browser window opened. Log in to X normally. When your timeline is visible, return here and press Enter.\n",
      "Cookies saved to x_cookies.json\n"
     ]
    }
   ],
   "source": [
    "# Run this ONCE if anonymous scraping yields 0 tweets.\n",
    "# It opens a real browser window. Log in to X, then press Enter in the notebook.\n",
    "async def login_and_save_cookies():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(\n",
    "            headless=False, args=[\"--disable-blink-features=AutomationControlled\"]\n",
    "        )\n",
    "        ctx = await browser.new_context(user_agent=UA, viewport={\"width\":1200,\"height\":900})\n",
    "        page = await ctx.new_page()\n",
    "        await page.goto(\"https://x.com/i/flow/login\", wait_until=\"domcontentloaded\", timeout=120_000)\n",
    "        print(\"A browser window opened. Log in to X normally. When your timeline is visible, return here and press Enter.\")\n",
    "        input()\n",
    "        cookies = await ctx.cookies()\n",
    "        with open(COOKIES_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cookies, f, ensure_ascii=False, indent=2)\n",
    "        await browser.close()\n",
    "        print(f\"Cookies saved to {COOKIES_FILE}\")\n",
    "\n",
    "# Usage (uncomment to run once):\n",
    "#await login_and_save_cookies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ada8561",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_last_hours(user: str, hours_back: int = 24):\n",
    "    cutoff = datetime.now(timezone.utc) - timedelta(hours=hours_back)\n",
    "    tz = ZoneInfo(OUTPUT_TZ)\n",
    "    rows, seen = [], set()\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        # Chromium is a bit more reliable on X; switch to firefox if you prefer\n",
    "        browser = await p.chromium.launch(\n",
    "            headless=True, args=[\"--disable-blink-features=AutomationControlled\"]\n",
    "        )\n",
    "        ctx = await browser.new_context(user_agent=UA, viewport={\"width\":1280,\"height\":2000})\n",
    "\n",
    "        # Reuse your logged-in session if available (recommended)\n",
    "        if Path(COOKIES_FILE).exists():\n",
    "            try:\n",
    "                with open(COOKIES_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                    await ctx.add_cookies(json.load(f))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        page = await ctx.new_page()\n",
    "        await page.goto(f\"https://x.com/{user}\", wait_until=\"domcontentloaded\", timeout=90_000)\n",
    "\n",
    "        # Try to dismiss consent/overlays (best effort)\n",
    "        for label in (\"Accept\", \"I agree\", \"Allow all\"):\n",
    "            try:\n",
    "                await page.get_by_role(\"button\", name=label).click(timeout=1500)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Scroll & collect\n",
    "        for _ in range(MAX_SCROLLS):\n",
    "            arts = await page.query_selector_all('article[data-testid=\"tweet\"]')\n",
    "            for a in arts:\n",
    "                link_el = await a.query_selector('a[role=\"link\"][href*=\"/status/\"]')\n",
    "                if not link_el:\n",
    "                    continue\n",
    "                href = await link_el.get_attribute(\"href\")\n",
    "                if not href or href in seen:\n",
    "                    continue\n",
    "\n",
    "                t_el = await a.query_selector(\"time\")\n",
    "                if not t_el:\n",
    "                    continue\n",
    "                dt_str = await t_el.get_attribute(\"datetime\")\n",
    "                if not dt_str:\n",
    "                    continue\n",
    "\n",
    "                # parse ISO timestamp (UTC), filter by cutoff\n",
    "                dt = datetime.fromisoformat(dt_str.replace(\"Z\", \"+00:00\"))\n",
    "                if dt < cutoff:\n",
    "                    continue\n",
    "\n",
    "                parts = await a.query_selector_all('[data-testid=\"tweetText\"]')\n",
    "                text = \"\\n\".join([await n.inner_text() for n in parts]).strip() if parts else \"\"\n",
    "\n",
    "                rows.append({\"time\": dt.astimezone(tz), \"text\": text})\n",
    "                seen.add(href)\n",
    "\n",
    "            # scroll for more\n",
    "            await page.mouse.wheel(0, 20000)\n",
    "            await page.wait_for_timeout(SCROLL_WAIT_MS)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    # sort newest → oldest & dedupe by (time, text)\n",
    "    out_map = {(r[\"time\"].isoformat(), r[\"text\"]): r for r in rows}\n",
    "    out = list(out_map.values())\n",
    "    out.sort(key=lambda r: r[\"time\"], reverse=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4be65939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 36 tweets in last 24h\n",
      "Saved -> financialjuice_last_hours.txt\n"
     ]
    }
   ],
   "source": [
    "rows = await scrape_last_hours(USER, HOURS_BACK)\n",
    "print(f\"Collected {len(rows)} tweets in last {HOURS_BACK}h\")\n",
    "\n",
    "with open(OUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in rows:\n",
    "        f.write(f\"{r['time'].strftime('%Y-%m-%d %H:%M:%S %Z')} | {r['text']}\\n\\n\")\n",
    "\n",
    "print(f\"Saved -> {OUT_TXT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a350de59",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Missing GOOGLE_API_KEY environment variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m GOOGLE_API_KEY = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mGOOGLE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m GOOGLE_API_KEY:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMissing GOOGLE_API_KEY environment variable.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m genai.configure(api_key=GOOGLE_API_KEY)\n\u001b[32m     11\u001b[39m MODEL_NAME = \u001b[33m\"\u001b[39m\u001b[33mgemini-1.5-flash\u001b[39m\u001b[33m\"\u001b[39m   \u001b[38;5;66;03m# or \"gemini-1.5-pro\"\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Missing GOOGLE_API_KEY environment variable."
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from typing import List, Tuple\n",
    "import pathlib\n",
    "\n",
    "# Set your key in the environment before running (e.g., in a terminal: export GOOGLE_API_KEY=\"...\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise RuntimeError(\"Missing GOOGLE_API_KEY environment variable.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "MODEL_NAME = \"gemini-1.5-flash\"   # or \"gemini-1.5-pro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55f3d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets(path: str = \"financialjuice_last_hours.txt\") -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "def chunk_text(s: str, max_chars: int = 15000) -> List[str]:\n",
    "    \"\"\"Greedy chunking by blank-line separated entries, staying under max_chars per chunk.\"\"\"\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\")\n",
    "    chunks, cur, total = [], [], 0\n",
    "    for block in s.split(\"\\n\\n\"):\n",
    "        block += \"\\n\\n\"\n",
    "        if cur and total + len(block) > max_chars:\n",
    "            chunks.append(\"\".join(cur))\n",
    "            cur, total = [], 0\n",
    "        cur.append(block)\n",
    "        total += len(block)\n",
    "    if cur:\n",
    "        chunks.append(\"\".join(cur))\n",
    "    return chunks\n",
    "\n",
    "def summarize_chunks(chunks: List[str], custom_prompt: str) -> List[str]:\n",
    "    model = genai.GenerativeModel(MODEL_NAME)\n",
    "    system = \"Follow the user’s instructions exactly. Do not add extra sections beyond what they ask.\"\n",
    "    outputs = []\n",
    "    for i, ch in enumerate(chunks, 1):\n",
    "        prompt = (\n",
    "            custom_prompt\n",
    "            + f\"\\n\\nCHUNK {i}/{len(chunks)} — TWEETS START\\n<<<\\n{ch}\\n>>>\\n\"\n",
    "            + \"Return a concise markdown summary (headings + bullet points).\"\n",
    "        )\n",
    "        resp = model.generate_content([system, prompt])\n",
    "        outputs.append((resp.text or \"\").strip())\n",
    "    return outputs\n",
    "\n",
    "def final_synthesis(per_chunk: List[str], custom_prompt: str) -> str:\n",
    "    model = genai.GenerativeModel(MODEL_NAME)\n",
    "    joined = \"\\n\\n--- CHUNK SPLIT ---\\n\\n\".join(per_chunk)\n",
    "    prompt = (\n",
    "        custom_prompt\n",
    "        + \"\\n\\nYou are given partial summaries of tweet batches. \"\n",
    "          \"Merge them into ONE well-structured Markdown document following the exact instructions above.\\n\\n\"\n",
    "          \"PARTIAL SUMMARIES START\\n<<<\\n\" + joined + \"\\n>>>\\n\"\n",
    "          \"Return ONLY the final markdown.\"\n",
    "    )\n",
    "    resp = model.generate_content(prompt)\n",
    "    return (resp.text or \"\").strip()\n",
    "\n",
    "def summarize_tweets_to_md(\n",
    "    tweets_path: str = \"financialjuice_last_hours.txt\",\n",
    "    output_md: str = \"summary.md\",\n",
    "    custom_prompt: str = \"\",\n",
    "    max_chars_per_chunk: int = 15000,\n",
    ") -> Tuple[str, int, int]:\n",
    "    raw = load_tweets(tweets_path)\n",
    "    if not raw:\n",
    "        raise ValueError(f\"Tweet file is empty: {tweets_path}\")\n",
    "\n",
    "    chunks = chunk_text(raw, max_chars=max_chars_per_chunk)\n",
    "    per_chunk = summarize_chunks(chunks, custom_prompt)\n",
    "    md = final_synthesis(per_chunk, custom_prompt)\n",
    "\n",
    "    pathlib.Path(output_md).write_text(md, encoding=\"utf-8\")\n",
    "    return output_md, len(raw), len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948d82c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = (\n",
    "    \"You are drafting a clear, trader‑ready summary. Be concise. \"\n",
    "    \"Group by **Macro Area** (in this order: Europe, UK, North America, Asia) and then **Countries** \"\n",
    "    \"Avoid repetition. Prefer bullet points over prose; keep each bullet short and factual. \"\n",
    "    \"If there is a news that is relevant for europe in general, or for many countries, place it under the **Europe**, without creating a new section. \"\n",
    "    \"Use the timestamps to resolve conflicts: if a newer tweet updates an older one, only keep the updated fact. \"\n",
    "    \"If a data release includes estimates/priors vs actuals, show the comparison succinctly. \"\n",
    "    \"Output must be valid Markdown.\"\n",
    ")\n",
    "\n",
    "out_file, n_chars, n_chunks = summarize_tweets_to_md(\n",
    "    tweets_path=OUT_TXT,\n",
    "    output_md=\"summary.md\",\n",
    "    custom_prompt=custom_prompt,\n",
    "    max_chars_per_chunk=15000,\n",
    ")\n",
    "\n",
    "print(f\"Done. Input chars: {n_chars}, chunks: {n_chunks}\")\n",
    "print(f\"Saved -> {out_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
